{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('torque_baseline.csv')\n",
    "df2 = pd.read_csv('torque_perturb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max normalization\n",
    "n_df1 = (df1-df1.min())/(df1.max()-df1.min())\n",
    "n_df2 = (df2-df2.min())/(df2.max()-df2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(n_df1, n_df2):\n",
    "    labels = []\n",
    "    l = len(n_df1) + len(n_df2)\n",
    "    concat_df = n_df1.append(n_df2, ignore_index=True)\n",
    "    shuffled_df = pd.DataFrame()\n",
    "    random_list  = list(range(l))\n",
    "    random.shuffle(random_list)\n",
    "    for num in random_list:\n",
    "        chunk_size = random.randint(5,21)\n",
    "        if num + chunk_size < l:\n",
    "            if num < len(n_df1):\n",
    "                labels.extend([0]*chunk_size)\n",
    "            else:\n",
    "                labels.extend([1]*chunk_size)\n",
    "            shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n",
    "    return shuffled_df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_df, labels = shuffle_data(n_df1, n_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = shuffled_df.loc[0:int(len(shuffled_df)*0.9)-1].to_numpy()\n",
    "# X_test = shuffled_df.loc[int(len(shuffled_df)*0.9):].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.array(labels[0:int(len(shuffled_df)*0.9)])\n",
    "# y_test = np.array(labels[int(len(shuffled_df)*0.9):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "# X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "# y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "# y_test_tensors = Variable(torch.Tensor(y_test)) \n",
    "\n",
    "# y_train_tensors = Variable(torch.Tensor(y_train).unsqueeze(1))\n",
    "# y_test_tensors = Variable(torch.Tensor(y_test).unsqueeze(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "# X_test_tensors = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    shuffled_df, labels = shuffle_data(n_df1, n_df2)\n",
    "    X_train = shuffled_df.loc[0:int(len(shuffled_df)*0.9)-1].to_numpy()\n",
    "    X_test = shuffled_df.loc[int(len(shuffled_df)*0.9):].to_numpy()\n",
    "    y_train = np.array(labels[0:int(len(shuffled_df)*0.9)])\n",
    "    y_test = np.array(labels[int(len(shuffled_df)*0.9):])\n",
    "    X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "    X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "    y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "    y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "    return X_train_tensors, X_test_tensors, y_train_tensors, y_test_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1908398/3894994880.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  concat_df = n_df1.append(n_df2, ignore_index=True)\n",
      "/tmp/ipykernel_1908398/3894994880.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors, X_test_tensors, y_train_tensors, y_test_tensors = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 #1000 epochs\n",
    "learning_rate = 1e-4 #0.001 lr\n",
    "\n",
    "input_size = 12 #number of features\n",
    "hidden_size = 10 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 #number of output classes \n",
    "\n",
    "lstm = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1908398/3894994880.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  concat_df = n_df1.append(n_df2, ignore_index=True)\n",
      "/tmp/ipykernel_1908398/3894994880.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.60213\n",
      "Epoch: 1, loss: 0.59660\n",
      "Epoch: 2, loss: 0.58995\n",
      "Epoch: 3, loss: 0.59472\n",
      "Epoch: 4, loss: 0.61494\n",
      "Epoch: 5, loss: 0.68795\n",
      "Epoch: 6, loss: 0.65858\n",
      "Epoch: 7, loss: 0.63618\n",
      "Epoch: 8, loss: 0.61014\n",
      "Epoch: 9, loss: 0.58182\n",
      "Epoch: 10, loss: 0.28713\n",
      "Epoch: 11, loss: 0.24177\n",
      "Epoch: 12, loss: 0.20025\n",
      "Epoch: 13, loss: 0.16270\n",
      "Epoch: 14, loss: 0.12792\n",
      "Epoch: 15, loss: 0.52684\n",
      "Epoch: 16, loss: 0.48973\n",
      "Epoch: 17, loss: 0.46531\n",
      "Epoch: 18, loss: 0.44194\n",
      "Epoch: 19, loss: 0.42369\n",
      "Epoch: 20, loss: 0.29915\n",
      "Epoch: 21, loss: 0.28906\n",
      "Epoch: 22, loss: 0.27760\n",
      "Epoch: 23, loss: 0.26617\n",
      "Epoch: 24, loss: 0.24427\n",
      "Epoch: 25, loss: 0.29705\n",
      "Epoch: 26, loss: 0.28115\n",
      "Epoch: 27, loss: 0.26533\n",
      "Epoch: 28, loss: 0.25167\n",
      "Epoch: 29, loss: 0.24050\n",
      "Epoch: 30, loss: 0.30422\n",
      "Epoch: 31, loss: 0.30646\n",
      "Epoch: 32, loss: 0.31422\n",
      "Epoch: 33, loss: 0.32370\n",
      "Epoch: 34, loss: 0.33348\n",
      "Epoch: 35, loss: 0.23211\n",
      "Epoch: 36, loss: 0.23453\n",
      "Epoch: 37, loss: 0.23550\n",
      "Epoch: 38, loss: 0.23687\n",
      "Epoch: 39, loss: 0.23762\n",
      "Epoch: 40, loss: 0.04031\n",
      "Epoch: 41, loss: 0.03845\n",
      "Epoch: 42, loss: 0.03689\n",
      "Epoch: 43, loss: 0.03526\n",
      "Epoch: 44, loss: 0.03400\n",
      "Epoch: 45, loss: 0.18116\n",
      "Epoch: 46, loss: 0.18033\n",
      "Epoch: 47, loss: 0.18020\n",
      "Epoch: 48, loss: 0.18111\n",
      "Epoch: 49, loss: 0.18181\n",
      "Epoch: 50, loss: 0.01950\n",
      "Epoch: 51, loss: 0.01836\n",
      "Epoch: 52, loss: 0.01735\n",
      "Epoch: 53, loss: 0.01636\n",
      "Epoch: 54, loss: 0.01551\n",
      "Epoch: 55, loss: 0.15768\n",
      "Epoch: 56, loss: 0.15013\n",
      "Epoch: 57, loss: 0.14452\n",
      "Epoch: 58, loss: 0.13739\n",
      "Epoch: 59, loss: 0.13135\n",
      "Epoch: 60, loss: 0.00665\n",
      "Epoch: 61, loss: 0.00550\n",
      "Epoch: 62, loss: 0.00486\n",
      "Epoch: 63, loss: 0.00435\n",
      "Epoch: 64, loss: 0.00388\n",
      "Epoch: 65, loss: 0.01032\n",
      "Epoch: 66, loss: 0.00960\n",
      "Epoch: 67, loss: 0.00819\n",
      "Epoch: 68, loss: 0.00729\n",
      "Epoch: 69, loss: 0.00557\n",
      "Epoch: 70, loss: 0.04202\n",
      "Epoch: 71, loss: 0.03482\n",
      "Epoch: 72, loss: 0.02558\n",
      "Epoch: 73, loss: 0.02338\n",
      "Epoch: 74, loss: 0.02139\n",
      "Epoch: 75, loss: 0.02578\n",
      "Epoch: 76, loss: 0.02918\n",
      "Epoch: 77, loss: 0.03148\n",
      "Epoch: 78, loss: 0.03337\n",
      "Epoch: 79, loss: 0.03476\n",
      "Epoch: 80, loss: 0.03395\n",
      "Epoch: 81, loss: 0.03271\n",
      "Epoch: 82, loss: 0.03196\n",
      "Epoch: 83, loss: 0.03128\n",
      "Epoch: 84, loss: 0.02994\n",
      "Epoch: 85, loss: 0.02012\n",
      "Epoch: 86, loss: 0.02086\n",
      "Epoch: 87, loss: 0.02118\n",
      "Epoch: 88, loss: 0.02144\n",
      "Epoch: 89, loss: 0.02160\n",
      "Epoch: 90, loss: 0.00935\n",
      "Epoch: 91, loss: 0.00915\n",
      "Epoch: 92, loss: 0.00907\n",
      "Epoch: 93, loss: 0.00899\n",
      "Epoch: 94, loss: 0.00907\n",
      "Epoch: 95, loss: 0.16970\n",
      "Epoch: 96, loss: 0.16417\n",
      "Epoch: 97, loss: 0.15867\n",
      "Epoch: 98, loss: 0.15412\n",
      "Epoch: 99, loss: 0.15010\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    if epoch % 5 == 0:\n",
    "        X_train_tensors, X_test_tensors, y_train_tensors, y_test_tensors = generate_data()\n",
    "    for i in range(len(X_test_tensors)):\n",
    "        input = torch.reshape(X_train_tensors[i], (1, 1, X_train_tensors[i].shape[0]))\n",
    "        outputs = lstm.forward(input).flatten() #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "        target = y_train_tensors[i].unsqueeze(0)\n",
    "        # obtain the loss function\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        \n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM1(\n",
       "  (lstm): LSTM(12, 10, batch_first=True)\n",
       "  (fc_1): Linear(in_features=10, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "threshold = torch.tensor([0.5])\n",
    "for i in range(len(X_test_tensors)):\n",
    "    input = torch.reshape(X_test_tensors[i], (1, 1, X_test_tensors[i].shape[0]))\n",
    "    output = lstm(input)\n",
    "    result = ((output>threshold).float()*1).flatten()\n",
    "    target = y_test_tensors[i].unsqueeze(0)\n",
    "    if torch.equal(result, target):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803098773402195"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(X_test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm, 'lstm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df1 = pd.read_csv('torque_baseline.csv')\n",
    "        self.df2 = pd.read_csv('torque_perturb.csv')\n",
    "        self.n_df1 = (self.df1-self.df1.min())/(self.df1.max()-self.df1.min())\n",
    "        self.n_df2 = (self.df2-self.df2.min())/(self.df2.max()-self.df2.min())\n",
    "        self.np_ndf1 = self.n_df1.to_numpy()\n",
    "        \n",
    "    def test_min_max(self):\n",
    "        zeros = []\n",
    "        for i in range(len(self.n_df1.columns)):\n",
    "            zeros.append(0)        \n",
    "        ones = []\n",
    "        for i in range(len(self.n_df1.columns)):\n",
    "            ones.append(0)\n",
    "        \n",
    "        for val in self.n_df1.max():\n",
    "            assert val == 1.0\n",
    "        \n",
    "        for val in self.n_df1.min():\n",
    "            assert val == 0.0\n",
    "\n",
    "        for val in self.n_df2.max():\n",
    "            assert val == 1.0\n",
    "\n",
    "        for val in self.n_df2.min():\n",
    "            assert val == 0.0\n",
    "    \n",
    "    def test_randlist(self):\n",
    "        random_list = list(range(len(self.df2)))\n",
    "        random.shuffle(random_list)\n",
    "        assert len(random_list) == len(self.df2)\n",
    "    \n",
    "    def test_concat(self):\n",
    "        l = len(self.df2) + len(self.df1)\n",
    "        concat_df = self.n_df1.append(self.n_df2, ignore_index=True)\n",
    "        assert len(concat_df) == l\n",
    "    \n",
    "    def test_shuffler(self):\n",
    "        label = []\n",
    "        l = len(self.df2) + len(self.df1)\n",
    "        concat_df = self.n_df1.append(self.n_df2, ignore_index=True)\n",
    "        shuffled_df = pd.DataFrame()\n",
    "        random_list  = list(range(l))\n",
    "        random.shuffle(random_list)\n",
    "        for num in random_list:\n",
    "            chunk_size = random.randint(5,21)\n",
    "            if num + chunk_size < l:\n",
    "                if num < len(self.df1):\n",
    "                    label.extend([0]*chunk_size)\n",
    "                else:\n",
    "                    label.extend([1]*chunk_size)\n",
    "                shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n",
    "        assert len(shuffled_df) > l\n",
    "        assert len(label) == len(shuffled_df)\n",
    "    \n",
    "    def test_TrainTest(self):\n",
    "        shuffled_df, labels = shuffle_data(n_df1, n_df2)\n",
    "        X_train = shuffled_df.loc[0:int(len(shuffled_df)*0.9)-1]\n",
    "        X_test = shuffled_df.loc[int(len(shuffled_df)*0.9):]\n",
    "        y_train = labels[0:int(len(shuffled_df)*0.9)]\n",
    "        y_test = labels[int(len(shuffled_df)*0.9):]\n",
    "        assert len(X_train) == len(y_train)\n",
    "        assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e041c755d760eef424980fce5d2929a22f217aef117a535d50142ca3e9f20fb3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
