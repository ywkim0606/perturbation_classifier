{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('torque_baseline.csv')\n",
    "df2 = pd.read_csv('torque_perturb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max normalization\n",
    "n_df1 = (df1-df1.min())/(df1.max()-df1.min())\n",
    "n_df2 = (df2-df2.min())/(df2.max()-df2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(n_df1, n_df2):\n",
    "    labels = []\n",
    "    l = len(n_df1) + len(n_df2)\n",
    "    concat_df = n_df1.append(n_df2, ignore_index=True)\n",
    "    shuffled_df = pd.DataFrame()\n",
    "    random_list  = list(range(l))\n",
    "    random.shuffle(random_list)\n",
    "    for num in random_list:\n",
    "        chunk_size = random.randint(5,21)\n",
    "        if num + chunk_size < l:\n",
    "            if num < len(n_df1):\n",
    "                labels.extend([0]*chunk_size)\n",
    "            else:\n",
    "                labels.extend([1]*chunk_size)\n",
    "            shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n",
    "    return shuffled_df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1889811/3894994880.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  concat_df = n_df1.append(n_df2, ignore_index=True)\n",
      "/tmp/ipykernel_1889811/3894994880.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "shuffled_df, labels = shuffle_data(n_df1, n_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = shuffled_df.loc[0:int(len(shuffled_df)*0.9)-1].to_numpy()\n",
    "X_test = shuffled_df.loc[int(len(shuffled_df)*0.9):].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(labels[0:int(len(shuffled_df)*0.9)])\n",
    "y_test = np.array(labels[int(len(shuffled_df)*0.9):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test)) \n",
    "\n",
    "# y_train_tensors = Variable(torch.Tensor(y_train).unsqueeze(1))\n",
    "# y_test_tensors = Variable(torch.Tensor(y_test).unsqueeze(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "# X_test_tensors = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30 #1000 epochs\n",
    "learning_rate = 1e-4 #0.001 lr\n",
    "\n",
    "input_size = 12 #number of features\n",
    "hidden_size = 2 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 #number of output classes \n",
    "\n",
    "lstm = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.62999\n",
      "Epoch: 1, loss: 0.62957\n",
      "Epoch: 2, loss: 0.62911\n",
      "Epoch: 3, loss: 0.63380\n",
      "Epoch: 4, loss: 0.64372\n",
      "Epoch: 5, loss: 0.66292\n",
      "Epoch: 6, loss: 0.69143\n",
      "Epoch: 7, loss: 0.71793\n",
      "Epoch: 8, loss: 0.74193\n",
      "Epoch: 9, loss: 0.75084\n",
      "Epoch: 10, loss: 0.74541\n",
      "Epoch: 11, loss: 0.72916\n",
      "Epoch: 12, loss: 0.70890\n",
      "Epoch: 13, loss: 0.68781\n",
      "Epoch: 14, loss: 0.66461\n",
      "Epoch: 15, loss: 0.64165\n",
      "Epoch: 16, loss: 0.62040\n",
      "Epoch: 17, loss: 0.60802\n",
      "Epoch: 18, loss: 0.58782\n",
      "Epoch: 19, loss: 0.57345\n",
      "Epoch: 20, loss: 0.56507\n",
      "Epoch: 21, loss: 0.55277\n",
      "Epoch: 22, loss: 0.54180\n",
      "Epoch: 23, loss: 0.52574\n",
      "Epoch: 24, loss: 0.51140\n",
      "Epoch: 25, loss: 0.49658\n",
      "Epoch: 26, loss: 0.48043\n",
      "Epoch: 27, loss: 0.46087\n",
      "Epoch: 28, loss: 0.44089\n",
      "Epoch: 29, loss: 0.41332\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i in range(len(X_test_tensors)):\n",
    "        input = torch.reshape(X_train_tensors[i], (1, 1, X_train_tensors[i].shape[0]))\n",
    "        outputs = lstm.forward(input).flatten() #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "        target = y_train_tensors[i].unsqueeze(0)\n",
    "        # obtain the loss function\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        \n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM1(\n",
       "  (lstm): LSTM(12, 2, batch_first=True)\n",
       "  (fc_1): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "threshold = torch.tensor([0.5])\n",
    "for i in range(len(X_test_tensors)):\n",
    "    input = torch.reshape(X_test_tensors[i], (1, 1, X_test_tensors[i].shape[0]))\n",
    "    output = lstm(input)\n",
    "    result = ((output>threshold).float()*1).flatten()\n",
    "    target = y_test_tensors[i].unsqueeze(0)\n",
    "    if torch.equal(result, target):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308776425368354"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(X_test_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df1 = pd.read_csv('torque_baseline.csv')\n",
    "        self.df2 = pd.read_csv('torque_perturb.csv')\n",
    "        self.n_df1 = (self.df1-self.df1.min())/(self.df1.max()-self.df1.min())\n",
    "        self.n_df2 = (self.df2-self.df2.min())/(self.df2.max()-self.df2.min())\n",
    "        self.np_ndf1 = self.n_df1.to_numpy()\n",
    "        \n",
    "    def test_min_max(self):\n",
    "        zeros = []\n",
    "        for i in range(len(self.n_df1.columns)):\n",
    "            zeros.append(0)        \n",
    "        ones = []\n",
    "        for i in range(len(self.n_df1.columns)):\n",
    "            ones.append(0)\n",
    "        \n",
    "        for val in self.n_df1.max():\n",
    "            assert val == 1.0\n",
    "        \n",
    "        for val in self.n_df1.min():\n",
    "            assert val == 0.0\n",
    "\n",
    "        for val in self.n_df2.max():\n",
    "            assert val == 1.0\n",
    "\n",
    "        for val in self.n_df2.min():\n",
    "            assert val == 0.0\n",
    "    \n",
    "    def test_randlist(self):\n",
    "        random_list = list(range(len(self.df2)))\n",
    "        random.shuffle(random_list)\n",
    "        assert len(random_list) == len(self.df2)\n",
    "    \n",
    "    def test_concat(self):\n",
    "        l = len(self.df2) + len(self.df1)\n",
    "        concat_df = self.n_df1.append(self.n_df2, ignore_index=True)\n",
    "        assert len(concat_df) == l\n",
    "    \n",
    "    def test_shuffler(self):\n",
    "        label = []\n",
    "        l = len(self.df2) + len(self.df1)\n",
    "        concat_df = self.n_df1.append(self.n_df2, ignore_index=True)\n",
    "        shuffled_df = pd.DataFrame()\n",
    "        random_list  = list(range(l))\n",
    "        random.shuffle(random_list)\n",
    "        for num in random_list:\n",
    "            chunk_size = random.randint(5,21)\n",
    "            if num + chunk_size < l:\n",
    "                if num < len(self.df1):\n",
    "                    label.extend([0]*chunk_size)\n",
    "                else:\n",
    "                    label.extend([1]*chunk_size)\n",
    "                shuffled_df = shuffled_df.append(concat_df.loc[num:num+chunk_size-1], ignore_index=True)\n",
    "        assert len(shuffled_df) > l\n",
    "        assert len(label) == len(shuffled_df)\n",
    "    \n",
    "    def test_TrainTest(self):\n",
    "        shuffled_df, labels = shuffle_data(n_df1, n_df2)\n",
    "        X_train = shuffled_df.loc[0:int(len(shuffled_df)*0.9)-1]\n",
    "        X_test = shuffled_df.loc[int(len(shuffled_df)*0.9):]\n",
    "        y_train = labels[0:int(len(shuffled_df)*0.9)]\n",
    "        y_test = labels[int(len(shuffled_df)*0.9):]\n",
    "        assert len(X_train) == len(y_train)\n",
    "        assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_TrainTest (__main__.TestStringMethods) ... ok\n",
      "test_concat (__main__.TestStringMethods) ... ok\n",
      "test_min_max (__main__.TestStringMethods) ... ok\n",
      "test_randlist (__main__.TestStringMethods) ... ok\n",
      "test_shuffler (__main__.TestStringMethods) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 2.373s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e041c755d760eef424980fce5d2929a22f217aef117a535d50142ca3e9f20fb3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
